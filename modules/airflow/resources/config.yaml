###################################
# Airflow - Common Configs
###################################
airflow:
  ## the airflow executor type to use
  ##
  executor: KubernetesExecutor

  ## the fernet key used to encrypt the connections in the database
  ##
  fernetKey: "7T512UXSSmBOkpWimFHIVb8jK6lfmSAvx4mO6Arehnc="

  ## environment variables for the web/scheduler/worker Pods (for airflow configs)
  ##
  config:
    ## security
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "False"

    ## dags
    AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    AIRFLOW__SCHEDULER__DAG_DIR_LIST_INTERVAL: "60"

    ## enable API
    AIRFLOW__API__AUTH_BACKEND: "airflow.api.auth.backend.basic_auth"

    AIRFLOW__LOGGING__REMOTE_LOGGING: "True"
    AIRFLOW__LOGGING__REMOTE_BASE_LOG_FOLDER: "gs://tbd-2021-airflow-logs/airflow/logs"
    AIRFLOW__LOGGING__REMOTE_LOG_CONN_ID: "google_cloud_default"

  ## a list of initial users to create
  ##
  users:
    - username: admin
      password: admin
      role: Admin
      email: admin@example.com
      firstName: admin
      lastName: admin
    - username: user
      password: user
      role: User
      email: user@example.com
      firstName: user
      lastName: user

  ## if we update users or just create them the first time (lookup by `username`)
  ##
  usersUpdate: false

  ## a list of initial connections to create
  ##
  connections:
    ## see docs: https://airflow.apache.org/docs/apache-airflow-providers-google/stable/connections/gcp.html
    - id: my_gcp
      type: google_cloud_platform
      description: my GCP connection
      extra: |-
        { "extra__google_cloud_platform__num_retries": "5" }
    - id: kubernetes_default
      type: kubernetes
      description: Default Kubernetes cluster connection
      namespace: default
      extra: |-
        { "extra__kubernetes__in_cluster": "True" }

  ## a list of initial variables to create
  ##
  variables:
    - key: "environment"
      value: "prod"

  ## a list of initial pools to create
  ##
  pools:
    - name: "pool_1"
      slots: 5
      description: "example pool with 3 slots"
    - name: "pool_2"
      slots: 10
      description: "example pool with 5 slots"

  extraPipPackages:
    - "airflow-exporter"
    - "apache-airflow-providers-cncf-kubernetes"
    - "prometheus_client"

###################################
# Airflow - Scheduler Configs
###################################
scheduler:
  ## the number of scheduler Pods to run
  ##
  replicas: 1
  ## resource requests/limits for the scheduler Pod
  ##
  resources:
    requests:
      cpu: "1000m"
      memory: "512Mi"

###################################
# Airflow - WebUI Configs
###################################
web:
  ## configs to generate webserver_config.py
  ##
  webserverConfig:
    ## the full text value to mount as the webserver_config.py file
    ##
    stringOverride: |-
      from flask_appbuilder.security.manager import AUTH_DB
      # use embedded DB for auth
      AUTH_TYPE = AUTH_DB
  ## the number of web Pods to run
  ##
  replicas: 1

  ## resource requests/limits for the airflow web Pods
  ##
  resources:
    requests:
      cpu: "200m"
      memory: "900Mi"

  ## configs for the web Pods' liveness probe
  ##
  livenessProbe:
    enabled: true
    initialDelaySeconds: 10
    periodSeconds: 10
    timeoutSeconds: 5
    failureThreshold: 6

###################################
# Airflow - Worker Configs
###################################
workers:
  ## if the airflow workers StatefulSet should be deployed
  ##
  enabled: false

###################################
# Airflow - Flower Configs
###################################
flower:
  ## if the Flower UI should be deployed
  ##
  enabled: false

###################################
# Airflow - Logs Configs
###################################
logs:
  ## configs for the logs PVC
  ##
  persistence:
    enabled: false

###################################
# Airflow - DAGs Configs
###################################
dags:
  ## the airflow dags folder
  ##
  path: /opt/airflow/dags

  gitSync:
    enabled: true
    repo: "git@github.com:miillo/thesis-dags.git"
    branch: "main"
    revision: "HEAD"
    syncWait: 60
    sshSecret: "airflow-ssh-git-secret"
    sshSecretKey: "id_rsa"
    sshKnownHosts: ""

###################################
# Kubernetes - RBAC
###################################
rbac:
  ## if Kubernetes RBAC resources are created
  ##
  create: true

###################################
# Kubernetes - Extra Manifests
###################################
## extra Kubernetes manifests to include alongside this chart
##
## NOTE:
## - this can be used to include ANY Kubernetes YAML resource
##
## EXAMPLE:
##   extraManifests:
##    - apiVersion: cloud.google.com/v1beta1
##      kind: BackendConfig
##      metadata:
##        name: "{{ .Release.Name }}-test"
##      spec:
##        securityPolicy:
##          name: "gcp-cloud-armor-policy-test"
##
extraManifests:
  - apiVersion: rbac.authorization.k8s.io/v1
    kind: ClusterRoleBinding
    metadata:
      name: airflow-spark-jobs
    subjects:
      - kind: ServiceAccount
        name: airflow
        namespace: default
    roleRef:
      kind: ClusterRole
      name: spark-operator
      apiGroup: rbac.authorization.k8s.io

###################################
# Kubernetes - Service Account
###################################
serviceAccount:
  ## if a Kubernetes ServiceAccount is created
  ##
  create: true

  ## the name of the ServiceAccount
  ##
  name: "airflow"

  ## annotations for the ServiceAccount
  ##
  #annotations:
  #  iam.gke.io/gcp-service-account: default@tbd-2021l-125.iam.gserviceaccount.com
  annotations:
    iam.gke.io/gcp-service-account: "tbd-lab@tbd-2021l-125.iam.gserviceaccount.com"

###################################
# Database - PostgreSQL Chart
###################################
postgresql:
  enabled: false

externalDatabase:
  ## the type of external database: {mysql,postgres}
  ##
  type: postgres

  ## the host of the external database
  ##
  host: tbd-airflow-external-db

  ## the port of the external database
  ##
  port: 5432

  ## the database/scheme to use within the the external database
  ##
  database: airflow_db

  ## the user of the external database
  ##
  user: airflow_user

  ## the name of a pre-created secret containing the external database password
  ##
  passwordSecret: "airflow-user.tbd-airflow-external-db.credentials.postgresql.acid.zalan.do"

  ## the key within `externalDatabase.passwordSecret` containing the password string
  ##
  passwordSecretKey: "password"

  ## the connection properties for external database, e.g. "?sslmode=require"
  properties: "?sslmode=require"

###################################
# Database - Redis Chart
###################################
redis:
  enabled: false

######################################
# Prometheus Operator - ServiceMonitor
######################################
serviceMonitor:
  ## if ServiceMonitor resources should be deployed for airflow webserver
  ##
  ## WARNING:
  ## - you will need an exporter in your airflow docker container, for example:
  ##   https://github.com/epoch8/airflow-exporter
  ##
  ## NOTE:
  ## - you can install pip packages with `airflow.extraPipPackages`
  ## - ServiceMonitor is a resource from: https://github.com/prometheus-operator/prometheus-operator
  ##
  enabled: true

  ## labels for ServiceMonitor, so that Prometheus can select it
  ##
  selector:
    prometheus: airflow-web

  ## the ServiceMonitor web endpoint path
  ##
  path: /admin/metrics

  ## the ServiceMonitor web endpoint interval
  ##
  interval: "30s"